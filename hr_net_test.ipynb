{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from post_processing.inference import get_final_preds\n",
    "from post_processing.transforms import get_affine_transform\n",
    "\n",
    "from pose_hr_net import get_pose_net\n",
    "\n",
    "from config.default_configuration import _C as cfg\n",
    "from config.default_configuration import update_config, COCO_INSTANCE_CATEGORY_NAMES, joints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def get_person_detection_boxes(model, img, threshold=0.5):\n",
    "    pil_image = Image.fromarray(img)  \n",
    "    transform = transforms.Compose([transforms.ToTensor()])  \n",
    "    transformed_img = transform(pil_image)  \n",
    "    pred = model([transformed_img])  \n",
    "    pred_classes = [COCO_INSTANCE_CATEGORY_NAMES[i]\n",
    "                    for i in list(pred[0]['labels'].numpy())]  \n",
    "    pred_boxes = [[(i[0], i[1]), (i[2], i[3])]\n",
    "                  for i in list(pred[0]['boxes'].detach().numpy())]  \n",
    "    pred_score = list(pred[0]['scores'].detach().numpy())\n",
    "    if not pred_score:\n",
    "        return []\n",
    "    \n",
    "    pred_t = [pred_score.index(x) for x in pred_score if x > threshold][-1]\n",
    "    pred_boxes = pred_boxes[:pred_t+1]\n",
    "    pred_classes = pred_classes[:pred_t+1]\n",
    "\n",
    "    person_boxes = []\n",
    "    for idx, box in enumerate(pred_boxes):\n",
    "        if pred_classes[idx] == 'person':\n",
    "            person_boxes.append(box)\n",
    "\n",
    "    return person_boxes\n",
    "\n",
    "\n",
    "def get_pose_estimation_prediction(pose_model, image, center, scale):\n",
    "    rotation = 0\n",
    "\n",
    "    trans = get_affine_transform(center, scale, rotation, cfg.MODEL.IMAGE_SIZE)\n",
    "    model_input = cv2.warpAffine(image, trans, (int(cfg.MODEL.IMAGE_SIZE[0]), int(cfg.MODEL.IMAGE_SIZE[1])),\n",
    "        flags=cv2.INTER_LINEAR)\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    model_input = transform(model_input).unsqueeze(0)\n",
    "    pose_model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = pose_model(model_input)\n",
    "        preds, _ = get_final_preds(\n",
    "            cfg,\n",
    "            output.clone().cpu().numpy(),\n",
    "            np.asarray([center]),\n",
    "            np.asarray([scale]))\n",
    "\n",
    "        return preds\n",
    "\n",
    "\n",
    "def box_to_center_scale(box, model_image_width, model_image_height):\n",
    "\n",
    "    center = np.zeros((2), dtype=np.float32)\n",
    "\n",
    "    bottom_left_corner = box[0]\n",
    "    top_right_corner = box[1]\n",
    "    box_width = top_right_corner[0]-bottom_left_corner[0]\n",
    "    box_height = top_right_corner[1]-bottom_left_corner[1]\n",
    "    bottom_left_x = bottom_left_corner[0]\n",
    "    bottom_left_y = bottom_left_corner[1]\n",
    "    center[0] = bottom_left_x + box_width * 0.5\n",
    "    center[1] = bottom_left_y + box_height * 0.5\n",
    "\n",
    "    aspect_ratio = model_image_width * 1.0 / model_image_height\n",
    "    pixel_std = 200\n",
    "\n",
    "    if box_width > aspect_ratio * box_height:\n",
    "        box_height = box_width * 1.0 / aspect_ratio\n",
    "    elif box_width < aspect_ratio * box_height:\n",
    "        box_width = box_height * aspect_ratio\n",
    "    scale = np.array([box_width * 1.0 / pixel_std, box_height * 1.0 / pixel_std], dtype=np.float32)\n",
    "    if center[0] != -1:\n",
    "        scale = scale * 1.25\n",
    "\n",
    "    return center, scale\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "pose_dir = 'outputs/poses/'\n",
    "box_dir = 'outputs/boxes/'\n",
    "images_dir = 'outputs/poses/'\n",
    "\n",
    "class Args:\n",
    "  cfg = 'additional_files/config/inference-config.yaml'\n",
    "  videoFile = 'data/spinning.mp4'\n",
    "  outputDir = 'outputs/'\n",
    "  inferenceFps = 10\n",
    "  writeBoxFrames = True\n",
    "  MODEL_FILE = 'additional_files/models/pytorch/pose_coco/pose_hrnet_w32_384x288.pth'\n",
    "  modelDir = ''\n",
    "  logDir = ''\n",
    "  dataDir = ''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# cudnn related setting\n",
    "cudnn.benchmark = cfg.CUDNN.BENCHMARK\n",
    "torch.backends.cudnn.deterministic = cfg.CUDNN.DETERMINISTIC\n",
    "torch.backends.cudnn.enabled = cfg.CUDNN.ENABLED"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "args=Args()\n",
    "update_config(cfg, args)\n",
    "\n",
    "box_model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "box_model.cuda()\n",
    "box_model.eval()\n",
    "\n",
    "pose_model = get_pose_net(cfg, is_train=False)\n",
    "pose_model.load_state_dict(torch.load(args.MODEL_FILE), strict=False)\n",
    "pose_model.cuda()\n",
    "pose_model = torch.nn.DataParallel(pose_model, device_ids=cfg.GPUS)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# original \n",
    "def run_video(input_video):\n",
    "    vidcap = cv2.VideoCapture(input_video)\n",
    "    fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
    "    if fps < args.inferenceFps:\n",
    "        print('desired inference fps is '+str(args.inferenceFps)+' but video fps is '+str(fps))\n",
    "        exit()\n",
    "    every_nth_frame = round(fps/args.inferenceFps)\n",
    "\n",
    "    success, image_bgr = vidcap.read()\n",
    "    count = 0\n",
    "    while success:\n",
    "        if count % every_nth_frame != 0:\n",
    "            success, image_bgr = vidcap.read()\n",
    "            count += 1\n",
    "            continue\n",
    "\n",
    "        image = image_bgr[:, :, [2, 1, 0]]\n",
    "        count_str = str(count).zfill(32)\n",
    "\n",
    "        # object detection box\n",
    "        pred_boxes = get_person_detection_boxes(box_model, image, threshold=0.8)\n",
    "\n",
    "        if not pred_boxes:\n",
    "            success, image_bgr = vidcap.read()\n",
    "            count += 1\n",
    "            continue\n",
    "\n",
    "        for box in pred_boxes:\n",
    "            # pose estimation\n",
    "            center, scale = box_to_center_scale(box, cfg.MODEL.IMAGE_SIZE[0], cfg.MODEL.IMAGE_SIZE[1])\n",
    "            image_pose = image.copy() if cfg.DATASET.COLOR_RGB else image_bgr.copy()\n",
    "            pose_preds = get_pose_estimation_prediction(pose_model, image_pose, center, scale)    \n",
    "\n",
    "            for _, mat in enumerate(pose_preds[0]):\n",
    "                x_coord, y_coord = int(mat[0]), int(mat[1])\n",
    "                cv2.circle(image_bgr, (x_coord, y_coord), 4, (255, 0, 0), 1)\n",
    "\n",
    "            for _, joint in enumerate(joints['coco']['skeleton']):\n",
    "                pt1, pt2 = pose_preds[0][joint]\n",
    "                cv2.line(image_bgr, (int(pt1[0]), int(pt1[1])), (int(pt2[0]), int(pt2[1])), (80, 80, 255), 1)\n",
    "\n",
    "            x,y,w,h = cv2.boundingRect(pose_preds[0])\n",
    "            cv2.rectangle(image_bgr, (x-10,y-10), (x+w+10,y+h+10), (80, 80, 255), thickness=1)            \n",
    "            cv2.rectangle(image_bgr, box[0], box[1], color=(180, 180, 0), thickness=1)\n",
    "\n",
    "        cv2.imwrite(pose_dir+'pose%s.jpg' % count_str, image_bgr)\n",
    "\n",
    "        # get next frame\n",
    "        success, image_bgr = vidcap.read()\n",
    "        count += 1\n",
    "\n",
    "    import imageio\n",
    "\n",
    "    images = []\n",
    "    for file_name in os.listdir(pose_dir):\n",
    "        if file_name.endswith('.jpg'):\n",
    "            file_path = os.path.join(pose_dir, file_name)\n",
    "            images.append(imageio.imread(file_path))\n",
    "    imageio.mimsave('outputs/movie.gif', images)    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "run_video(args.videoFile)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# # single image\n",
    "# def run(image_bgr):\n",
    "#     image = image_bgr[:, :, [2, 1, 0]]\n",
    "#     # object detection box\n",
    "#     pred_boxes = get_person_detection_boxes(box_model, image, threshold=0.8)\n",
    "#     image_bgr_box = image_bgr.copy()\n",
    "#     for box in pred_boxes:\n",
    "#         cv2.rectangle(image_bgr_box, box[0], box[1], color=(0, 255, 0), thickness=1)\n",
    "#         if pred_boxes:\n",
    "#             # pose estimation\n",
    "#             center, scale = box_to_center_scale(box, cfg.MODEL.IMAGE_SIZE[0], cfg.MODEL.IMAGE_SIZE[1])\n",
    "#             image_pose = image.copy() if cfg.DATASET.COLOR_RGB else image_bgr.copy()\n",
    "#             pose_preds = get_pose_estimation_prediction(pose_model, image_pose, center, scale)    \n",
    "# \n",
    "#             for _, mat in enumerate(pose_preds[0]):\n",
    "#                 x_coord, y_coord = int(mat[0]), int(mat[1])\n",
    "#                 cv2.circle(image_bgr, (x_coord, y_coord), 4, (255, 0, 0), 1)\n",
    "# \n",
    "#             for i, joint in enumerate(joints['coco']['skeleton']):\n",
    "#                 pt1, pt2 = pose_preds[0][joint]\n",
    "#                 cv2.line(image_bgr, (int(pt1[0]), int(pt1[1])), (int(pt2[0]), int(pt2[1])), (80, 80, 255), 1)\n",
    "# \n",
    "#             x,y,w,h = cv2.boundingRect(pose_preds[0])\n",
    "#             cv2.rectangle(image_bgr, (x-10,y-10), (x+w+10,y+h+10), (80, 80, 255), thickness=1)            \n",
    "#             cv2.rectangle(image_bgr, box[0], box[1], color=(180, 180, 0), thickness=1)\n",
    "# \n",
    "#     cv2.imwrite('single_image.png', image_bgr)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "elapsed time: 19.432769060134888s\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# import time\n",
    "# \n",
    "# image_bgr = cv2.imread('data/22.png')\n",
    "# \n",
    "# start_time = time.time()\n",
    "# run(image_bgr)\n",
    "# stop_time = time.time()\n",
    "# print('elapsed time: {}s'.format(stop_time-start_time))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}